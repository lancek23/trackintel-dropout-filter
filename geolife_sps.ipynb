{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0fc3a09",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01e9676",
   "metadata": {},
   "outputs": [],
   "source": [
    "import trackintel as ti\n",
    "import glob\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import os\n",
    "import random\n",
    "\n",
    "from trackintel.preprocessing import generate_locations, generate_staypoints, generate_triplegs, merge_staypoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd40779",
   "metadata": {},
   "source": [
    "User parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636e873d",
   "metadata": {},
   "outputs": [],
   "source": [
    "timezone = 'Asia/Shanghai'\n",
    "\n",
    "data_path = 'Geolife Trajectories 1.3/Data/'\n",
    "\n",
    "# Set dropout threshold (should match min staypoint duration)\n",
    "dropout_threshold = pd.Timedelta(minutes=10)\n",
    "# Set time difference threshold between dropout/staypoint start here\n",
    "diff_threshold = pd.Timedelta(minutes=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29463ec3",
   "metadata": {},
   "source": [
    "Generate initial staypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58269bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = os.listdir(data_path)\n",
    "print(len(agents))\n",
    "print(agents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61642c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a single plt file for an agent\n",
    "def extract_day(file_path):\n",
    "    day = pd.read_csv(file_path, names=['latitude', 'longitude', 'zero', 'altitude', 'days', 'date', 'time'], skiprows=6)\n",
    "    day['tracked_at'] = pd.to_datetime(day['date'] + ' ' + day['time']).dt.tz_localize(timezone)\n",
    "    day = day.drop(columns=['zero', 'altitude', 'days', 'date', 'time'])\n",
    "\n",
    "    user_id = file_path[file_path.index('Data')+5:file_path.index('Data')+8]\n",
    "    day['user_id'] = user_id\n",
    "\n",
    "    return day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd8ed41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract raw trajectory\n",
    "def get_full_traj(agent_path):\n",
    "    agent_trajs = glob.glob(agent_path+'Trajectory/*.plt')\n",
    "    agent_traj = pd.concat([extract_day(x) for x in agent_trajs], ignore_index=True)\n",
    "    agent_traj = agent_traj.sort_values(by=['tracked_at']).reset_index(drop=True)\n",
    "\n",
    "    # Convert to geo dataframe\n",
    "    agent_traj = gpd.GeoDataFrame(agent_traj, geometry=gpd.points_from_xy(agent_traj['longitude'], agent_traj['latitude'], crs=\"EPSG:4326\"))\n",
    "\n",
    "    return agent_traj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5b4b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because the Geolife data is often much larger and more complex than the T-drive data, we will\n",
    "# employ a few more tools of the trackintel library to generate high quality initial staypoints\n",
    "\n",
    "def get_ti_sps(path, timezone='Asia/Shanghai'):\n",
    "    df = get_full_traj(path)\n",
    "    if len(df) > 1:\n",
    "        # WHAT THIS DOES: will return pfs as the original df but with assigned staypoint id (or NA) to each point\n",
    "        #                 generates initial sps\n",
    "        # Currently using simple parameters\n",
    "        pfs, sps = generate_staypoints(\n",
    "            df,\n",
    "            dist_threshold=100, # Min dist between staypoints, in meters\n",
    "            time_threshold=pd.Timedelta(minutes=10), # Min duration to create a staypoint\n",
    "            gap_threshold=pd.Timedelta(minutes=25), # Max gap time to still mark something as a staypoint\n",
    "            include_last=True, # Makes sure we include the last one if the user ends there\n",
    "        )\n",
    "\n",
    "        # I believe this is the only place we should need this, since we may return no staypoints\n",
    "        if len(sps) == 0:\n",
    "            return\n",
    "\n",
    "        # WHAT THIS DOES: Generates triplegs (which we'll need later) by just looking between sps\n",
    "        #                 returns pfs as initial pfs but with assigned tripleg id (or NA) to each point. Now, every point should be at a sp or tripleg\n",
    "        # Taking same gap threshold as in generate_staypoints\n",
    "        pfs, tpls = generate_triplegs(pfs, gap_threshold=25)\n",
    "\n",
    "        # WHAT THIS DOES: Adds a location id to each sp so we can make merging staypoints easier\n",
    "        #                 Could also return locations but we ignore it with underscore\n",
    "        # Taking default parameters\n",
    "        sps, _ = generate_locations(sps)\n",
    "\n",
    "        # WHAT THIS DOES: Merges staypoints that are at the same location, consecutive, and within some time gap of each other\n",
    "        # Setting max time gap to same as gap threshold; agg is necessary to include geometry, and takes last geometry of merged staypoints\n",
    "        sps = merge_staypoints(sps, tpls, max_time_gap=pd.Timedelta(minutes=25), agg={\"geometry\":\"last\"})\n",
    "    \n",
    "        if len(sps) > 0:\n",
    "            user_id=sps['user_id'].iloc[0]\n",
    "            sps.to_csv('geolife_sps/user_'+str(user_id)+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34291b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for agent in agents:\n",
    "    print(agent)\n",
    "    get_ti_sps(data_path+agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead21923",
   "metadata": {},
   "source": [
    "Generate trajectories with dropouts and staypoints based on these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529fbd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want to create on average 8 dropouts. Thus, probability is 8 / len(df). This number is variable\n",
    "def get_dirty_ti_sps(path, avg_num_dropouts=8):\n",
    "    df = get_full_traj(path)\n",
    "    if len(df) == 0:\n",
    "        return\n",
    "    \n",
    "    # Create on average 8 dropouts\n",
    "    dropoutlength = pd.Timedelta(minutes=15) # Can also change dropout time if desired\n",
    "    dropouts = []\n",
    "    i = 0\n",
    "    while i < len(df):\n",
    "        if random.random() > avg_num_dropouts / len(df):\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        j = i\n",
    "        while j < len(df) and (df['tracked_at'].iloc[j] - df['tracked_at'].iloc[i]) < dropoutlength:\n",
    "            j += 1\n",
    "        \n",
    "        dropouts.append([i, j])\n",
    "\n",
    "        i = j\n",
    "    \n",
    "    all_dropouts = []\n",
    "    for dropout in dropouts:\n",
    "        all_dropouts.extend(list(range(dropout[0], dropout[1])))\n",
    "    \n",
    "    # Write indices of dropouts to a text file\n",
    "    with open('geolife_dropout_indices/user_'+str(df['user_id'].iloc[0])+'.txt', 'w') as f:\n",
    "        for line in all_dropouts:\n",
    "            f.write(f\"{line}\\n\")\n",
    "\n",
    "    # Create a list of start times we dropped for that agent\n",
    "    lines = [df['tracked_at'].iloc[x[0]] for x in dropouts]\n",
    "    with open('geolife_dropouts/user_'+str(df['user_id'].iloc[0])+'.txt', 'w') as f:\n",
    "        for line in lines:\n",
    "            f.write(f\"{line}\\n\")\n",
    "\n",
    "    df = df.drop(all_dropouts).reset_index(drop=True)\n",
    "\n",
    "    # We have to make this check again after adding dropouts\n",
    "    if len(df) == 0:\n",
    "        return\n",
    "\n",
    "    # Rerun trackintel on the trajectory with dropouts\n",
    "    pfs, sps = generate_staypoints(\n",
    "        df,\n",
    "        dist_threshold=100, # Min dist between staypoints, in meters\n",
    "        time_threshold=pd.Timedelta(minutes=10), # Min duration to create a staypoint\n",
    "        gap_threshold=pd.Timedelta(minutes=25), # Max gap time to still mark something as a staypoint\n",
    "        include_last=True, # Makes sure we include the last one if the user ends there\n",
    "    )\n",
    "    \n",
    "    if len(sps) == 0:\n",
    "        return\n",
    "\n",
    "    pfs, tpls = generate_triplegs(pfs, gap_threshold=25)\n",
    "\n",
    "    sps, _ = generate_locations(sps)\n",
    "\n",
    "    sps = merge_staypoints(sps, tpls, max_time_gap=pd.Timedelta(minutes=25), agg={\"geometry\":\"last\"})\n",
    "\n",
    "    # Write to file\n",
    "    if len(sps) > 0:\n",
    "        user_id=sps['user_id'].iloc[0]\n",
    "        sps.to_csv('geolife_noisy_sps/user_'+str(user_id)+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6cd5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for agent in agents:\n",
    "    print(agent)\n",
    "    get_dirty_ti_sps(data_path+agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b2abef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a flag for whether the staypoint is spurious\n",
    "def add_is_spurious(path):\n",
    "    noisy_sps = ti.read_staypoints_csv(path, index_col=None, geom_col='geometry')\n",
    "    user_id = noisy_sps['user_id'].iloc[0]\n",
    "\n",
    "    with open(\"geolife_dropouts/user_\"+str(user_id).zfill(3)+\".txt\", \"r\") as file:\n",
    "        dropouts = [line.strip() for line in file]\n",
    "    dropouts = pd.Series(dropouts)\n",
    "    dropouts = pd.to_datetime(dropouts)\n",
    "\n",
    "    noisy_sps['is_spurious'] = False\n",
    "    for i in dropouts:\n",
    "        abs_diff = abs(noisy_sps['started_at'] - i)\n",
    "        abs_diff = abs_diff <= diff_threshold\n",
    "        noisy_sps['is_spurious'] = noisy_sps['is_spurious'] | abs_diff\n",
    "    \n",
    "    noisy_sps.to_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a9f11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "noised_files = glob.glob('geolife_noisy_sps/*.csv')\n",
    "for nf in noised_files:\n",
    "    add_is_spurious(nf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77b3603",
   "metadata": {},
   "source": [
    "Now apply filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1903ef11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user(agent_id):\n",
    "    path = data_path + str(agent_id).zfill(3)\n",
    "    df = get_full_traj(path)\n",
    "    df['tracked_at']=df['tracked_at'].dt.tz_convert(timezone)\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_noisy_user(agent_id):\n",
    "    path = data_path + str(agent_id).zfill(3)\n",
    "    df = get_full_traj(path)\n",
    "    df['tracked_at']=df['tracked_at'].dt.tz_convert(timezone)\n",
    "\n",
    "    with open('geolife_dropout_indices/user_'+str(agent_id).zfill(3)+'.txt') as file:\n",
    "        all_dropouts = [int(line.strip()) for line in file]\n",
    "    \n",
    "    df = df.drop(all_dropouts).reset_index(drop=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecdecf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dropouts(agent_id):\n",
    "\n",
    "    # Identify dropouts over dropout threshold\n",
    "    traj = get_noisy_user(agent_id)\n",
    "    traj['t_delta'] = traj['tracked_at'].diff(1)\n",
    "    traj['t_delta'] = traj['t_delta'].shift(-1)\n",
    "\n",
    "    dropouts = traj[traj['t_delta'] >= dropout_threshold].reset_index(drop=True)\n",
    "    timezone = 'UTC+08:00'\n",
    "    dropouts['tracked_at']=dropouts['tracked_at'].dt.tz_convert(timezone)\n",
    "\n",
    "    noisy_sps = ti.read_staypoints_csv('geolife_noisy_sps/user_'+str(agent_id).zfill(3)+'.csv', index_col=None, geom_col='geometry')\n",
    "\n",
    "    # This function call identifies the spurious staypoints\n",
    "    merged = pd.merge_asof(\n",
    "        noisy_sps,\n",
    "        dropouts,\n",
    "        left_on=\"started_at\",\n",
    "        right_on=\"tracked_at\",\n",
    "        tolerance=diff_threshold,\n",
    "        direction=\"forward\"\n",
    "    )\n",
    "\n",
    "    # And here we remove these staypoints\n",
    "    indices_to_drop = noisy_sps.index[merged[\"tracked_at\"].notna()]\n",
    "    non_dropout_sps = noisy_sps.drop(indices_to_drop).reset_index(drop=True)\n",
    "\n",
    "    file_path = 'geolife_sps/user_'+str(agent_id).zfill(3)+'.csv'\n",
    "    if os.path.exists(file_path):\n",
    "        sps = ti.read_staypoints_csv(file_path, index_col=None, geom_col='geometry')\n",
    "    else:\n",
    "        sps = pd.DataFrame() # Should just be able to create an empty df I think\n",
    "\n",
    "    # We return:    Length of original staypoints, \n",
    "    #               length of noised staypoints, \n",
    "    #               length of spurious noised staypoints,\n",
    "    #               length of filtered staypoints\n",
    "    #               length of filtered spurious staypoints\n",
    "    \n",
    "    to_return = [agent_id,\n",
    "                 len(sps), \n",
    "                 len(noisy_sps), \n",
    "                 len(noisy_sps[noisy_sps['is_spurious'] == True]), \n",
    "                 len(non_dropout_sps), \n",
    "                 len(non_dropout_sps[non_dropout_sps['is_spurious'] == True])]\n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b11a738",
   "metadata": {},
   "source": [
    "Check results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca677e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=['user_id', 'num_sps', 'num_noised_sps', 'num_spurious_sps', 'num_filtered_sps', 'num_spurious_filtered_sps'])\n",
    "\n",
    "users = glob.glob('geolife_noisy_sps/*.csv')\n",
    "users = [int(user[user.index('user') + 5 : user.index('.')]) for user in users]\n",
    "\n",
    "for user in users:\n",
    "    results.loc[len(results)] = filter_dropouts(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b97b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can specify an output location\n",
    "results.to_csv('geolife_results.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "t-drive",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
